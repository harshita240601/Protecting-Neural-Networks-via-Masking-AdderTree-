{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qApfBD3acBSh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import argparse\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=False,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "ds_train = ds_train.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(batch_size)\n",
        "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test = ds_test.batch(batch_size)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Flatten the data\n",
        "def flatten_image(image, label):\n",
        "    return tf.reshape(image, (-1, 28*28)), label\n",
        "\n",
        "# Apply flatten function to train and test datasets\n",
        "ds_train = ds_train.map(flatten_image)\n",
        "ds_test = ds_test.map(flatten_image)\n"
      ],
      "metadata": {
        "id": "kelUYj_DcrpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **original weights:**"
      ],
      "metadata": {
        "id": "AuFEzoXCowx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 32, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "dSrQfTKHpRWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "E3rG1ImRoq2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ehsCt52otJ0",
        "outputId": "7865b0cd-f697-4ef7-cf5f-4a073302e27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6ms/step - accuracy: 0.8685 - loss: 0.4993 - val_accuracy: 0.9612 - val_loss: 0.1260\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - accuracy: 0.9695 - loss: 0.1063 - val_accuracy: 0.9688 - val_loss: 0.1085\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - accuracy: 0.9795 - loss: 0.0666 - val_accuracy: 0.9766 - val_loss: 0.0743\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - accuracy: 0.9863 - loss: 0.0464 - val_accuracy: 0.9772 - val_loss: 0.0723\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - accuracy: 0.9897 - loss: 0.0340 - val_accuracy: 0.9733 - val_loss: 0.0843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Updating weights of Layer 1:**"
      ],
      "metadata": {
        "id": "3wzi5kLJmSsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [8 , 4 , 2]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "2bumO897cPzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rcC_huVdh6hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# Initialize random weights for the first layer only\n",
        "first_layer_weights = initial_weights[0]  # First layer weights\n",
        "first_layer_biases = initial_weights[1]   # First layer biases\n",
        "\n",
        "\n",
        "# Set random values for the first layer weights using numpy\n",
        "random_weights = np.random.rand(*first_layer_weights.shape)\n",
        "\n",
        "# Update the first layer weights to the random values while keeping the biases the same\n",
        "initial_weights[0] = random_weights  # Replace the first layer's weights with random values\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)"
      ],
      "metadata": {
        "id": "Rp-ielauei7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_layer_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZlMPBG7e8JV",
        "outputId": "0ec20e47-755c-4bce-c381-4995c03cf03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_layer_biases.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsXRLT2HfMdo",
        "outputId": "ec0b2e3b-b95b-4ce3-fe71-b6c9b7c79feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.build()\n",
        "\n",
        "# Printing the input and output dimensions of each layer\n",
        "for i, layer in enumerate(model.layers):\n",
        "    weights_shape = layer.get_weights()[0].shape  # Shape of weight matrix (input_dim, output_dim)\n",
        "    biases_shape = layer.get_weights()[1].shape   # Shape of bias vector (output_dim,)\n",
        "    print(f\"Layer {i+1}:\")\n",
        "    print(f\"  Input shape: {weights_shape[0]}, Output shape: {weights_shape[1]}\")\n",
        "    print(f\"  Weight matrix shape: {weights_shape}\")\n",
        "    print(f\"  Bias vector shape: {biases_shape}\\n\")"
      ],
      "metadata": {
        "id": "dvOBpQbii21J",
        "outputId": "272b9744-eb98-4a50-a18d-f1a2b1bf2eff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1:\n",
            "  Input shape: 784, Output shape: 256\n",
            "  Weight matrix shape: (784, 256)\n",
            "  Bias vector shape: (256,)\n",
            "\n",
            "Layer 2:\n",
            "  Input shape: 256, Output shape: 32\n",
            "  Weight matrix shape: (256, 32)\n",
            "  Bias vector shape: (32,)\n",
            "\n",
            "Layer 3:\n",
            "  Input shape: 32, Output shape: 10\n",
            "  Weight matrix shape: (32, 10)\n",
            "  Bias vector shape: (10,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "9fzZzACzmoYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzP80Iq6mvO3",
        "outputId": "a22e2932-e061-4f5c-d684-f4f3d1b0ba0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.1072 - loss: 2.3095 - val_accuracy: 0.1135 - val_loss: 2.3064\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - accuracy: 0.1062 - loss: 2.3068 - val_accuracy: 0.1032 - val_loss: 2.3077\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.1044 - loss: 2.3068 - val_accuracy: 0.1032 - val_loss: 2.3067\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 6ms/step - accuracy: 0.1028 - loss: 2.3070 - val_accuracy: 0.1135 - val_loss: 2.3036\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - accuracy: 0.1024 - loss: 2.3070 - val_accuracy: 0.1135 - val_loss: 2.3148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Updating weights of Layer 2:**"
      ],
      "metadata": {
        "id": "9Mv6nbwKng5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "zF1LNMMsq-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# Initialize random weights for the second layer only\n",
        "second_layer_weights = initial_weights[2]  # Second layer weights\n",
        "second_layer_biases = initial_weights[3]   # Second layer biases\n",
        "\n",
        "# Set random values for the second layer weights using numpy\n",
        "random_weights = np.random.rand(*second_layer_weights.shape)\n",
        "\n",
        "# Update the second layer weights to the random values while keeping the biases the same\n",
        "initial_weights[2] = random_weights  # Replace the second layer's weights with random values\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n",
        "\n",
        "# Verifying the modified weights for the second layer\n",
        "# print(\"Updated second layer weights with random values:\")\n",
        "# print(initial_weights[2])  # Print the modified second layer weights"
      ],
      "metadata": {
        "id": "so-_HxktixJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "XCCTSCEAnrtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8nHDAyLnrg1",
        "outputId": "540ec568-4d4c-4dfa-f407-d03f7fd92fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.1043 - loss: 2.3234 - val_accuracy: 0.1028 - val_loss: 2.3080\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - accuracy: 0.1022 - loss: 2.3159 - val_accuracy: 0.0982 - val_loss: 2.3258\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - accuracy: 0.1023 - loss: 2.3181 - val_accuracy: 0.0958 - val_loss: 2.3098\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.1027 - loss: 2.3177 - val_accuracy: 0.0974 - val_loss: 2.3294\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.1056 - loss: 2.3161 - val_accuracy: 0.1028 - val_loss: 2.3121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Updating weights of Layer 3:**"
      ],
      "metadata": {
        "id": "O_PRJ8fAjv5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "iJTJJHqOsEAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c32cbc6-fd0d-4579-da1d-95d30ae9c8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# # Initialize random weights for the third layer only\n",
        "third_layer_weights = initial_weights[4]  # Third layer weights\n",
        "third_layer_biases = initial_weights[5]   # Third layer biases\n",
        "\n",
        "# # Set random values for the third layer weights using numpy\n",
        "random_weights = np.random.randn(*third_layer_weights.shape)\n",
        "\n",
        "# # Update the third layer weights to the random values while keeping the biases the same\n",
        "initial_weights[4] = random_weights  # Replace the third layer's weights with random values\n",
        "\n",
        "# # Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n",
        "\n",
        "# # Verifying the modified weights for the third layer\n",
        "# print(\"Updated third layer weights with random values:\")\n",
        "# print(initial_weights[4])  # Print the modified third layer weights"
      ],
      "metadata": {
        "id": "1hedxkPBjr3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "KhbtPwLNdCLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS-0b5FTjCds",
        "outputId": "df51158f-86fb-4354-de2c-d8be2af84411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.8781 - loss: 0.4080 - val_accuracy: 0.9673 - val_loss: 0.1094\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.9713 - loss: 0.0949 - val_accuracy: 0.9733 - val_loss: 0.0869\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.9811 - loss: 0.0617 - val_accuracy: 0.9737 - val_loss: 0.0898\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.9859 - loss: 0.0432 - val_accuracy: 0.9755 - val_loss: 0.0852\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - accuracy: 0.9897 - loss: 0.0313 - val_accuracy: 0.9738 - val_loss: 0.0877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hamming distance 2 on layer 1(exponent):**"
      ],
      "metadata": {
        "id": "SJe3Lusq4gcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "-6g_QWgW8P0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# Function to decompose a float into sign, exponent, and mantissa\n",
        "def decompose_float(value):\n",
        "\n",
        "    # Convert float to 64-bit binary\n",
        "    binary_repr = format(struct.unpack('!Q', struct.pack('!d', value))[0], '064b')\n",
        "    sign = binary_repr[0]\n",
        "    exponent = binary_repr[1:12]  # 11 bits for exponent in IEEE 754\n",
        "    mantissa = binary_repr[12:]   # 52 bits for mantissa\n",
        "    return sign, exponent, mantissa\n",
        "\n",
        "# Function to reconstruct a float from sign, exponent, and mantissa\n",
        "def reconstruct_float(sign, exponent, mantissa):\n",
        "    # Reconstruct the binary string\n",
        "    binary_repr = sign + exponent + mantissa\n",
        "    # Convert back to float\n",
        "    return struct.unpack('!d', struct.pack('!Q', int(binary_repr, 2)))[0]\n",
        "\n",
        "# Function to flip bits in the exponent part to achieve Hamming distance\n",
        "def flip_bits_exponent(exponent_str, n_flips):\n",
        "    # Convert the exponent string to a list of characters\n",
        "    exponent_list = list(exponent_str)\n",
        "\n",
        "    # Randomly select n_flips positions to flip\n",
        "    flip_indices = np.random.choice(len(exponent_list), n_flips, replace=False)\n",
        "\n",
        "    # Flip the bits at the selected indices\n",
        "    for idx in flip_indices:\n",
        "        exponent_list[idx] = '1' if exponent_list[idx] == '0' else '0'\n",
        "\n",
        "    # Join the list back into a string\n",
        "    return ''.join(exponent_list)\n",
        "\n",
        "# Function to modify weights by flipping bits in the exponent only\n",
        "def modify_exponent_hamming_distance(weights, hamming_distance):\n",
        "    # Flatten the weights to easily manipulate them\n",
        "    flat_weights = weights.flatten()\n",
        "\n",
        "    # Modify each weight to ensure a Hamming distance on the exponent\n",
        "    for i in range(len(flat_weights)):\n",
        "        # Decompose the float into sign, exponent, and mantissa\n",
        "        sign, exponent, mantissa = decompose_float(flat_weights[i])\n",
        "\n",
        "        # Flip bits in the exponent to achieve the specified Hamming distance\n",
        "        modified_exponent = flip_bits_exponent(exponent, hamming_distance)\n",
        "\n",
        "        # Reconstruct the float using the modified exponent and original sign/mantissa\n",
        "        flat_weights[i] = reconstruct_float(sign, modified_exponent, mantissa)\n",
        "\n",
        "    # Reshape back to original shape\n",
        "    return flat_weights.reshape(weights.shape)\n",
        "\n",
        "# Modify only the weights of the first layer (layer 1)\n",
        "layer_index = 0  # Index of the first layer (0-based index)\n",
        "initial_weights[layer_index * 2] = modify_exponent_hamming_distance(initial_weights[layer_index * 2], hamming_distance=2)\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttd3kbtEU4_E",
        "outputId": "d8ab12a3-a585-4bb2-ea71-8df6b673c3b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-9e54dc0a62b8>:50: RuntimeWarning: overflow encountered in cast\n",
            "  flat_weights[i] = reconstruct_float(sign, modified_exponent, mantissa)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "g4buM_LK8Tcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiBnVxFz8Vre",
        "outputId": "ae546fc8-166f-4f21-80f5-ef5cb2d95281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.0999 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 8ms/step - accuracy: 0.0982 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 6ms/step - accuracy: 0.0998 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - accuracy: 0.0986 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.0981 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hamming distance 2 on layer 1(mantissa):**"
      ],
      "metadata": {
        "id": "I_23BCZrehr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq1SNWnqfGj3",
        "outputId": "083eb5fa-17a0-472f-932c-030dbb452b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "\n",
        "# Function to convert float to IEEE 754 binary (sign, exponent, mantissa)\n",
        "def float_to_ieee754(value):\n",
        "    # Pack the float as a binary string (64-bit)\n",
        "    packed = struct.pack('>d', value)\n",
        "    # Convert to 64-bit unsigned integer\n",
        "    unpacked = struct.unpack('>Q', packed)[0]\n",
        "\n",
        "    # Extract sign (1 bit), exponent (11 bits), and mantissa (52 bits)\n",
        "    sign = (unpacked >> 63) & 1\n",
        "    exponent = (unpacked >> 52) & 0x7FF\n",
        "    mantissa = unpacked & ((1 << 52) - 1)\n",
        "\n",
        "    return sign, exponent, mantissa\n",
        "\n",
        "# Function to convert IEEE 754 binary (sign, exponent, mantissa) back to float\n",
        "def ieee754_to_float(sign, exponent, mantissa):\n",
        "    # Reassemble the binary representation\n",
        "    binary_rep = (sign << 63) | (exponent << 52) | mantissa\n",
        "    # Convert back to float\n",
        "    packed = struct.pack('>Q', binary_rep)\n",
        "    return struct.unpack('>d', packed)[0]\n",
        "\n",
        "# Function to flip bits in the mantissa to achieve exact Hamming distance\n",
        "def flip_bits_in_mantissa(mantissa, n_flips):\n",
        "    # Convert mantissa to a binary string\n",
        "    mantissa_bin = list(f'{mantissa:052b}')\n",
        "\n",
        "    # Randomly select n_flips positions to flip\n",
        "    flip_indices = np.random.choice(len(mantissa_bin), n_flips, replace=False)\n",
        "\n",
        "    # Flip the bits at the selected indices\n",
        "    for idx in flip_indices:\n",
        "        mantissa_bin[idx] = '1' if mantissa_bin[idx] == '0' else '0'\n",
        "\n",
        "    # Join the list back into a string and convert to integer\n",
        "    return int(''.join(mantissa_bin), 2)\n",
        "\n",
        "# Function to modify weights by flipping bits in the mantissa\n",
        "def modify_weights_mantissa(weights, hamming_distance):\n",
        "    # Flatten the weights to easily manipulate them\n",
        "    flat_weights = weights.flatten()\n",
        "\n",
        "    # Modify each weight's mantissa\n",
        "    for i in range(len(flat_weights)):\n",
        "        # Convert the weight to IEEE 754 components\n",
        "        sign, exponent, mantissa = float_to_ieee754(flat_weights[i])\n",
        "\n",
        "        # Flip bits in the mantissa to achieve the specified Hamming distance\n",
        "        modified_mantissa = flip_bits_in_mantissa(mantissa, hamming_distance)\n",
        "\n",
        "        # Convert the modified IEEE 754 components back to a float\n",
        "        flat_weights[i] = ieee754_to_float(sign, exponent, modified_mantissa)\n",
        "\n",
        "    # Reshape back to the original shape\n",
        "    return flat_weights.reshape(weights.shape)\n",
        "\n",
        "# Modify only the weights of the first layer (layer 1)\n",
        "layer_index = 0  # Index of the first layer (0-based index)\n",
        "initial_weights[layer_index * 2] = modify_weights_mantissa(initial_weights[layer_index * 2], hamming_distance=2)\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n"
      ],
      "metadata": {
        "id": "C6nd8gX5eeDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "ev7OkJ13f12t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhsk2Fppf2m1",
        "outputId": "aa4ef5e3-b963-4272-a018-27948c5096a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6ms/step - accuracy: 0.8700 - loss: 0.4545 - val_accuracy: 0.9615 - val_loss: 0.1266\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step - accuracy: 0.9676 - loss: 0.1059 - val_accuracy: 0.9687 - val_loss: 0.1036\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.9805 - loss: 0.0653 - val_accuracy: 0.9709 - val_loss: 0.0926\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 6ms/step - accuracy: 0.9862 - loss: 0.0459 - val_accuracy: 0.9765 - val_loss: 0.0737\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - accuracy: 0.9887 - loss: 0.0342 - val_accuracy: 0.9726 - val_loss: 0.0923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hamming distance 4 on layer 1(exponent):**"
      ],
      "metadata": {
        "id": "8wpf6vG0kyTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "HtYzt5uCkRW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# Function to decompose a float into sign, exponent, and mantissa\n",
        "def decompose_float(value):\n",
        "\n",
        "    # Convert float to 64-bit binary\n",
        "    binary_repr = format(struct.unpack('!Q', struct.pack('!d', value))[0], '064b')\n",
        "    sign = binary_repr[0]\n",
        "    exponent = binary_repr[1:12]  # 11 bits for exponent in IEEE 754\n",
        "    mantissa = binary_repr[12:]   # 52 bits for mantissa\n",
        "    return sign, exponent, mantissa\n",
        "\n",
        "# Function to reconstruct a float from sign, exponent, and mantissa\n",
        "def reconstruct_float(sign, exponent, mantissa):\n",
        "    # Reconstruct the binary string\n",
        "    binary_repr = sign + exponent + mantissa\n",
        "    # Convert back to float\n",
        "    return struct.unpack('!d', struct.pack('!Q', int(binary_repr, 2)))[0]\n",
        "\n",
        "# Function to flip bits in the exponent part to achieve Hamming distance\n",
        "def flip_bits_exponent(exponent_str, n_flips):\n",
        "    # Convert the exponent string to a list of characters\n",
        "    exponent_list = list(exponent_str)\n",
        "\n",
        "    # Randomly select n_flips positions to flip\n",
        "    flip_indices = np.random.choice(len(exponent_list), n_flips, replace=False)\n",
        "\n",
        "    # Flip the bits at the selected indices\n",
        "    for idx in flip_indices:\n",
        "        exponent_list[idx] = '1' if exponent_list[idx] == '0' else '0'\n",
        "\n",
        "    # Join the list back into a string\n",
        "    return ''.join(exponent_list)\n",
        "\n",
        "# Function to modify weights by flipping bits in the exponent only\n",
        "def modify_exponent_hamming_distance(weights, hamming_distance):\n",
        "    # Flatten the weights to easily manipulate them\n",
        "    flat_weights = weights.flatten()\n",
        "\n",
        "    # Modify each weight to ensure a Hamming distance on the exponent\n",
        "    for i in range(len(flat_weights)):\n",
        "        # Decompose the float into sign, exponent, and mantissa\n",
        "        sign, exponent, mantissa = decompose_float(flat_weights[i])\n",
        "\n",
        "        # Flip bits in the exponent to achieve the specified Hamming distance\n",
        "        modified_exponent = flip_bits_exponent(exponent, hamming_distance)\n",
        "\n",
        "        # Reconstruct the float using the modified exponent and original sign/mantissa\n",
        "        flat_weights[i] = reconstruct_float(sign, modified_exponent, mantissa)\n",
        "\n",
        "    # Reshape back to original shape\n",
        "    return flat_weights.reshape(weights.shape)\n",
        "\n",
        "# Modify only the weights of the second layer (layer 2)\n",
        "layer_index = 0  # Index of the second layer (0-based index)\n",
        "initial_weights[layer_index * 2] = modify_exponent_hamming_distance(initial_weights[layer_index * 2], hamming_distance=4)\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1dyye8ylchC",
        "outputId": "ab6dbbfd-a3ce-4a33-b700-5f87bca9004d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-ae26e29065cc>:50: RuntimeWarning: overflow encountered in cast\n",
            "  flat_weights[i] = reconstruct_float(sign, modified_exponent, mantissa)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "NcJsccpnldjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEd46MWdlmBV",
        "outputId": "ccb41289-669e-4668-8f1c-ba65081d371d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.0996 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.1015 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.1004 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 7ms/step - accuracy: 0.1003 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - accuracy: 0.0999 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hamming distance 4 on layer 1(mantissa):**"
      ],
      "metadata": {
        "id": "zvtIBfmVR_NW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "fvlTKBX2TCgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "\n",
        "# Function to convert float to IEEE 754 binary (sign, exponent, mantissa)\n",
        "def float_to_ieee754(value):\n",
        "    # Pack the float as a binary string (64-bit)\n",
        "    packed = struct.pack('>d', value)\n",
        "    # Convert to 64-bit unsigned integer\n",
        "    unpacked = struct.unpack('>Q', packed)[0]\n",
        "\n",
        "    # Extract sign (1 bit), exponent (11 bits), and mantissa (52 bits)\n",
        "    sign = (unpacked >> 63) & 1\n",
        "    exponent = (unpacked >> 52) & 0x7FF\n",
        "    mantissa = unpacked & ((1 << 52) - 1)\n",
        "\n",
        "    return sign, exponent, mantissa\n",
        "\n",
        "# Function to convert IEEE 754 binary (sign, exponent, mantissa) back to float\n",
        "def ieee754_to_float(sign, exponent, mantissa):\n",
        "    # Reassemble the binary representation\n",
        "    binary_rep = (sign << 63) | (exponent << 52) | mantissa\n",
        "    # Convert back to float\n",
        "    packed = struct.pack('>Q', binary_rep)\n",
        "    return struct.unpack('>d', packed)[0]\n",
        "\n",
        "# Function to flip bits in the mantissa to achieve exact Hamming distance\n",
        "def flip_bits_in_mantissa(mantissa, n_flips):\n",
        "    # Convert mantissa to a binary string\n",
        "    mantissa_bin = list(f'{mantissa:052b}')\n",
        "\n",
        "    # Randomly select n_flips positions to flip\n",
        "    flip_indices = np.random.choice(len(mantissa_bin), n_flips, replace=False)\n",
        "\n",
        "    # Flip the bits at the selected indices\n",
        "    for idx in flip_indices:\n",
        "        mantissa_bin[idx] = '1' if mantissa_bin[idx] == '0' else '0'\n",
        "\n",
        "    # Join the list back into a string and convert to integer\n",
        "    return int(''.join(mantissa_bin), 2)\n",
        "\n",
        "# Function to modify weights by flipping bits in the mantissa\n",
        "def modify_weights_mantissa(weights, hamming_distance):\n",
        "    # Flatten the weights to easily manipulate them\n",
        "    flat_weights = weights.flatten()\n",
        "\n",
        "    # Modify each weight's mantissa\n",
        "    for i in range(len(flat_weights)):\n",
        "        # Convert the weight to IEEE 754 components\n",
        "        sign, exponent, mantissa = float_to_ieee754(flat_weights[i])\n",
        "\n",
        "        # Flip bits in the mantissa to achieve the specified Hamming distance\n",
        "        modified_mantissa = flip_bits_in_mantissa(mantissa, hamming_distance)\n",
        "\n",
        "        # Convert the modified IEEE 754 components back to a float\n",
        "        flat_weights[i] = ieee754_to_float(sign, exponent, modified_mantissa)\n",
        "\n",
        "    # Reshape back to the original shape\n",
        "    return flat_weights.reshape(weights.shape)\n",
        "\n",
        "# Modify only the weights of the second layer (layer 2)\n",
        "layer_index = 0  # Index of the second layer (0-based index)\n",
        "initial_weights[layer_index * 2] = modify_weights_mantissa(initial_weights[layer_index * 2], hamming_distance=4)\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n"
      ],
      "metadata": {
        "id": "3dHD06XWTHsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "guHNlIFTTUqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfduovgnTXNL",
        "outputId": "4b08613a-3780-4961-c622-19cf295035ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - accuracy: 0.8714 - loss: 0.4520 - val_accuracy: 0.9649 - val_loss: 0.1158\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - accuracy: 0.9680 - loss: 0.1042 - val_accuracy: 0.9712 - val_loss: 0.0937\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - accuracy: 0.9796 - loss: 0.0650 - val_accuracy: 0.9720 - val_loss: 0.0891\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 7ms/step - accuracy: 0.9851 - loss: 0.0459 - val_accuracy: 0.9762 - val_loss: 0.0788\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - accuracy: 0.9895 - loss: 0.0341 - val_accuracy: 0.9767 - val_loss: 0.0776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hamming distance 8 on layer 1(exponent):**"
      ],
      "metadata": {
        "id": "SbQ0BXyuUrYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "V0CEl6_eTx-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# Function to decompose a float into sign, exponent, and mantissa\n",
        "def decompose_float(value):\n",
        "\n",
        "    # Convert float to 64-bit binary\n",
        "    binary_repr = format(struct.unpack('!Q', struct.pack('!d', value))[0], '064b')\n",
        "    sign = binary_repr[0]\n",
        "    exponent = binary_repr[1:12]  # 11 bits for exponent in IEEE 754\n",
        "    mantissa = binary_repr[12:]   # 52 bits for mantissa\n",
        "    return sign, exponent, mantissa\n",
        "\n",
        "# Function to reconstruct a float from sign, exponent, and mantissa\n",
        "def reconstruct_float(sign, exponent, mantissa):\n",
        "    # Reconstruct the binary string\n",
        "    binary_repr = sign + exponent + mantissa\n",
        "    # Convert back to float\n",
        "    return struct.unpack('!d', struct.pack('!Q', int(binary_repr, 2)))[0]\n",
        "\n",
        "# Function to flip bits in the exponent part to achieve Hamming distance\n",
        "def flip_bits_exponent(exponent_str, n_flips):\n",
        "    # Convert the exponent string to a list of characters\n",
        "    exponent_list = list(exponent_str)\n",
        "\n",
        "    # Randomly select n_flips positions to flip\n",
        "    flip_indices = np.random.choice(len(exponent_list), n_flips, replace=False)\n",
        "\n",
        "    # Flip the bits at the selected indices\n",
        "    for idx in flip_indices:\n",
        "        exponent_list[idx] = '1' if exponent_list[idx] == '0' else '0'\n",
        "\n",
        "    # Join the list back into a string\n",
        "    return ''.join(exponent_list)\n",
        "\n",
        "# Function to modify weights by flipping bits in the exponent only\n",
        "def modify_exponent_hamming_distance(weights, hamming_distance):\n",
        "    # Flatten the weights to easily manipulate them\n",
        "    flat_weights = weights.flatten()\n",
        "\n",
        "    # Modify each weight to ensure a Hamming distance on the exponent\n",
        "    for i in range(len(flat_weights)):\n",
        "        # Decompose the float into sign, exponent, and mantissa\n",
        "        sign, exponent, mantissa = decompose_float(flat_weights[i])\n",
        "\n",
        "        # Flip bits in the exponent to achieve the specified Hamming distance\n",
        "        modified_exponent = flip_bits_exponent(exponent, hamming_distance)\n",
        "\n",
        "        # Reconstruct the float using the modified exponent and original sign/mantissa\n",
        "        flat_weights[i] = reconstruct_float(sign, modified_exponent, mantissa)\n",
        "\n",
        "    # Reshape back to original shape\n",
        "    return flat_weights.reshape(weights.shape)\n",
        "\n",
        "# Modify only the weights of the second layer (layer 2)\n",
        "layer_index = 0  # Index of the second layer (0-based index)\n",
        "initial_weights[layer_index * 2] = modify_exponent_hamming_distance(initial_weights[layer_index * 2], hamming_distance=8)\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgXdktmDWUpC",
        "outputId": "50975b3f-dd3e-49a1-c7e9-6367ab578592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-cf45ab36ec14>:50: RuntimeWarning: overflow encountered in cast\n",
            "  flat_weights[i] = reconstruct_float(sign, modified_exponent, mantissa)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "GBYGC54rWb54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SadB9yIWilR",
        "outputId": "e0d067fa-6071-44dd-da91-45b9c92fb949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.0982 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.0995 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.0980 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - accuracy: 0.0974 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.0994 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hamming distance 8 on layer 1(mantissa):**"
      ],
      "metadata": {
        "id": "hp5EMJs2WowZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "_5DRXXPyWrbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "\n",
        "# Function to convert float to IEEE 754 binary (sign, exponent, mantissa)\n",
        "def float_to_ieee754(value):\n",
        "    # Pack the float as a binary string (64-bit)\n",
        "    packed = struct.pack('>d', value)\n",
        "    # Convert to 64-bit unsigned integer\n",
        "    unpacked = struct.unpack('>Q', packed)[0]\n",
        "\n",
        "    # Extract sign (1 bit), exponent (11 bits), and mantissa (52 bits)\n",
        "    sign = (unpacked >> 63) & 1\n",
        "    exponent = (unpacked >> 52) & 0x7FF\n",
        "    mantissa = unpacked & ((1 << 52) - 1)\n",
        "\n",
        "    return sign, exponent, mantissa\n",
        "\n",
        "# Function to convert IEEE 754 binary (sign, exponent, mantissa) back to float\n",
        "def ieee754_to_float(sign, exponent, mantissa):\n",
        "    # Reassemble the binary representation\n",
        "    binary_rep = (sign << 63) | (exponent << 52) | mantissa\n",
        "    # Convert back to float\n",
        "    packed = struct.pack('>Q', binary_rep)\n",
        "    return struct.unpack('>d', packed)[0]\n",
        "\n",
        "# Function to flip bits in the mantissa to achieve exact Hamming distance\n",
        "def flip_bits_in_mantissa(mantissa, n_flips):\n",
        "    # Convert mantissa to a binary string\n",
        "    mantissa_bin = list(f'{mantissa:052b}')\n",
        "\n",
        "    # Randomly select n_flips positions to flip\n",
        "    flip_indices = np.random.choice(len(mantissa_bin), n_flips, replace=False)\n",
        "\n",
        "    # Flip the bits at the selected indices\n",
        "    for idx in flip_indices:\n",
        "        mantissa_bin[idx] = '1' if mantissa_bin[idx] == '0' else '0'\n",
        "\n",
        "    # Join the list back into a string and convert to integer\n",
        "    return int(''.join(mantissa_bin), 2)\n",
        "\n",
        "# Function to modify weights by flipping bits in the mantissa\n",
        "def modify_weights_mantissa(weights, hamming_distance):\n",
        "    # Flatten the weights to easily manipulate them\n",
        "    flat_weights = weights.flatten()\n",
        "\n",
        "    # Modify each weight's mantissa\n",
        "    for i in range(len(flat_weights)):\n",
        "        # Convert the weight to IEEE 754 components\n",
        "        sign, exponent, mantissa = float_to_ieee754(flat_weights[i])\n",
        "\n",
        "        # Flip bits in the mantissa to achieve the specified Hamming distance\n",
        "        modified_mantissa = flip_bits_in_mantissa(mantissa, hamming_distance)\n",
        "\n",
        "        # Convert the modified IEEE 754 components back to a float\n",
        "        flat_weights[i] = ieee754_to_float(sign, exponent, modified_mantissa)\n",
        "\n",
        "    # Reshape back to the original shape\n",
        "    return flat_weights.reshape(weights.shape)\n",
        "\n",
        "# Modify only the weights of the second layer (layer 2)\n",
        "layer_index = 0  # Index of the second layer (0-based index)\n",
        "initial_weights[layer_index * 2] = modify_weights_mantissa(initial_weights[layer_index * 2], hamming_distance=8)\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n"
      ],
      "metadata": {
        "id": "zqEA4t6ZWt1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "2hBXWc21W13P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEjgK9lvW4CE",
        "outputId": "ba841fce-6762-4ba2-e285-99d2b1592ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.8748 - loss: 0.4522 - val_accuracy: 0.9620 - val_loss: 0.1255\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 6ms/step - accuracy: 0.9690 - loss: 0.1014 - val_accuracy: 0.9686 - val_loss: 0.0986\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - accuracy: 0.9794 - loss: 0.0681 - val_accuracy: 0.9731 - val_loss: 0.0832\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - accuracy: 0.9855 - loss: 0.0451 - val_accuracy: 0.9766 - val_loss: 0.0781\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - accuracy: 0.9897 - loss: 0.0336 - val_accuracy: 0.9768 - val_loss: 0.0775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hamming distance 2 on layer 2(exponent):**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jcN7tUXyXbkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "5T0EwHZlX7qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# Function to decompose a float into sign, exponent, and mantissa\n",
        "def decompose_float(value):\n",
        "\n",
        "    # Convert float to 64-bit binary\n",
        "    binary_repr = format(struct.unpack('!Q', struct.pack('!d', value))[0], '064b')\n",
        "    sign = binary_repr[0]\n",
        "    exponent = binary_repr[1:12]  # 11 bits for exponent in IEEE 754\n",
        "    mantissa = binary_repr[12:]   # 52 bits for mantissa\n",
        "    return sign, exponent, mantissa\n",
        "\n",
        "# Function to reconstruct a float from sign, exponent, and mantissa\n",
        "def reconstruct_float(sign, exponent, mantissa):\n",
        "    # Reconstruct the binary string\n",
        "    binary_repr = sign + exponent + mantissa\n",
        "    # Convert back to float\n",
        "    return struct.unpack('!d', struct.pack('!Q', int(binary_repr, 2)))[0]\n",
        "\n",
        "# Function to flip bits in the exponent part to achieve Hamming distance\n",
        "def flip_bits_exponent(exponent_str, n_flips):\n",
        "    # Convert the exponent string to a list of characters\n",
        "    exponent_list = list(exponent_str)\n",
        "\n",
        "    # Randomly select n_flips positions to flip\n",
        "    flip_indices = np.random.choice(len(exponent_list), n_flips, replace=False)\n",
        "\n",
        "    # Flip the bits at the selected indices\n",
        "    for idx in flip_indices:\n",
        "        exponent_list[idx] = '1' if exponent_list[idx] == '0' else '0'\n",
        "\n",
        "    # Join the list back into a string\n",
        "    return ''.join(exponent_list)\n",
        "\n",
        "# Function to modify weights by flipping bits in the exponent only\n",
        "def modify_exponent_hamming_distance(weights, hamming_distance):\n",
        "    # Flatten the weights to easily manipulate them\n",
        "    flat_weights = weights.flatten()\n",
        "\n",
        "    # Modify each weight to ensure a Hamming distance on the exponent\n",
        "    for i in range(len(flat_weights)):\n",
        "        # Decompose the float into sign, exponent, and mantissa\n",
        "        sign, exponent, mantissa = decompose_float(flat_weights[i])\n",
        "\n",
        "        # Flip bits in the exponent to achieve the specified Hamming distance\n",
        "        modified_exponent = flip_bits_exponent(exponent, hamming_distance)\n",
        "\n",
        "        # Reconstruct the float using the modified exponent and original sign/mantissa\n",
        "        flat_weights[i] = reconstruct_float(sign, modified_exponent, mantissa)\n",
        "\n",
        "    # Reshape back to original shape\n",
        "    return flat_weights.reshape(weights.shape)\n",
        "\n",
        "# Modify only the weights of the second layer (layer 2)\n",
        "layer_index = 1  # Index of the second layer (0-based index)\n",
        "initial_weights[layer_index * 2] = modify_exponent_hamming_distance(initial_weights[layer_index * 2], hamming_distance=2)\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYlOcjymX8XX",
        "outputId": "a4ab5b86-c681-4737-e88c-ac97ddbbedc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-cab7a8298e08>:50: RuntimeWarning: overflow encountered in cast\n",
            "  flat_weights[i] = reconstruct_float(sign, modified_exponent, mantissa)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "3wHkiF9iYHBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9f6EkHpYHkM",
        "outputId": "2cf923c5-978a-44a3-a6af-7e4d00e111e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.0998 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 6ms/step - accuracy: 0.0967 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 7ms/step - accuracy: 0.0991 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - accuracy: 0.0996 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.0990 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hamming distance 2 on layer 2(mantissa):**\n"
      ],
      "metadata": {
        "id": "b24PZ5HoYKHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "P-aIUkQZYPb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "\n",
        "# Function to convert float to IEEE 754 binary (sign, exponent, mantissa)\n",
        "def float_to_ieee754(value):\n",
        "    # Pack the float as a binary string (64-bit)\n",
        "    packed = struct.pack('>d', value)\n",
        "    # Convert to 64-bit unsigned integer\n",
        "    unpacked = struct.unpack('>Q', packed)[0]\n",
        "\n",
        "    # Extract sign (1 bit), exponent (11 bits), and mantissa (52 bits)\n",
        "    sign = (unpacked >> 63) & 1\n",
        "    exponent = (unpacked >> 52) & 0x7FF\n",
        "    mantissa = unpacked & ((1 << 52) - 1)\n",
        "\n",
        "    return sign, exponent, mantissa\n",
        "\n",
        "# Function to convert IEEE 754 binary (sign, exponent, mantissa) back to float\n",
        "def ieee754_to_float(sign, exponent, mantissa):\n",
        "    # Reassemble the binary representation\n",
        "    binary_rep = (sign << 63) | (exponent << 52) | mantissa\n",
        "    # Convert back to float\n",
        "    packed = struct.pack('>Q', binary_rep)\n",
        "    return struct.unpack('>d', packed)[0]\n",
        "\n",
        "# Function to flip bits in the mantissa to achieve exact Hamming distance\n",
        "def flip_bits_in_mantissa(mantissa, n_flips):\n",
        "    # Convert mantissa to a binary string\n",
        "    mantissa_bin = list(f'{mantissa:052b}')\n",
        "\n",
        "    # Randomly select n_flips positions to flip\n",
        "    flip_indices = np.random.choice(len(mantissa_bin), n_flips, replace=False)\n",
        "\n",
        "    # Flip the bits at the selected indices\n",
        "    for idx in flip_indices:\n",
        "        mantissa_bin[idx] = '1' if mantissa_bin[idx] == '0' else '0'\n",
        "\n",
        "    # Join the list back into a string and convert to integer\n",
        "    return int(''.join(mantissa_bin), 2)\n",
        "\n",
        "# Function to modify weights by flipping bits in the mantissa\n",
        "def modify_weights_mantissa(weights, hamming_distance):\n",
        "    # Flatten the weights to easily manipulate them\n",
        "    flat_weights = weights.flatten()\n",
        "\n",
        "    # Modify each weight's mantissa\n",
        "    for i in range(len(flat_weights)):\n",
        "        # Convert the weight to IEEE 754 components\n",
        "        sign, exponent, mantissa = float_to_ieee754(flat_weights[i])\n",
        "\n",
        "        # Flip bits in the mantissa to achieve the specified Hamming distance\n",
        "        modified_mantissa = flip_bits_in_mantissa(mantissa, hamming_distance)\n",
        "\n",
        "        # Convert the modified IEEE 754 components back to a float\n",
        "        flat_weights[i] = ieee754_to_float(sign, exponent, modified_mantissa)\n",
        "\n",
        "    # Reshape back to the original shape\n",
        "    return flat_weights.reshape(weights.shape)\n",
        "\n",
        "# Modify only the weights of the second layer (layer 2)\n",
        "layer_index = 0  # Index of the second layer (0-based index)\n",
        "initial_weights[layer_index * 2] = modify_weights_mantissa(initial_weights[layer_index * 2], hamming_distance=16)\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n"
      ],
      "metadata": {
        "id": "41G7ECoPYVF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "p0DFwWmBYhkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfsjQFF-YlvB",
        "outputId": "9e72c9a5-a685-45f5-f117-89a21f4f319d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - accuracy: 0.8736 - loss: 0.4512 - val_accuracy: 0.9614 - val_loss: 0.1207\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9700 - loss: 0.0990 - val_accuracy: 0.9749 - val_loss: 0.0886\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.9797 - loss: 0.0663 - val_accuracy: 0.9743 - val_loss: 0.0768\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0419 - val_accuracy: 0.9778 - val_loss: 0.0802\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 5ms/step - accuracy: 0.9887 - loss: 0.0373 - val_accuracy: 0.9768 - val_loss: 0.0768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hamming distance 4 on layer 2(exponent):**\n"
      ],
      "metadata": {
        "id": "QuKdQI31ZTk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import struct\n",
        "seed = 42\n",
        "tf.keras.utils.set_random_seed(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "layer_dims = [784 , 256 , 64, 10]\n",
        "model = Sequential()\n",
        "input_dim = layer_dims[0]  # Extracting the input layer dimension\n",
        "# Adding the input layer separately\n",
        "model.add(Dense(layer_dims[1], activation='sigmoid', input_dim=input_dim))\n",
        "# Adding the rest of the layers\n",
        "if len(layer_dims) > 2:\n",
        "    # Adding the rest of the layers\n",
        "    for dim in layer_dims[2:]:\n",
        "        model.add(Dense(dim, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "bxWib5_YZbb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the initial weights of the model\n",
        "initial_weights = model.get_weights()\n",
        "\n",
        "# Function to decompose a float into sign, exponent, and mantissa\n",
        "def decompose_float(value):\n",
        "\n",
        "    # Convert float to 64-bit binary\n",
        "    binary_repr = format(struct.unpack('!Q', struct.pack('!d', value))[0], '064b')\n",
        "    sign = binary_repr[0]\n",
        "    exponent = binary_repr[1:12]  # 11 bits for exponent in IEEE 754\n",
        "    mantissa = binary_repr[12:]   # 52 bits for mantissa\n",
        "    return sign, exponent, mantissa\n",
        "\n",
        "# Function to reconstruct a float from sign, exponent, and mantissa\n",
        "def reconstruct_float(sign, exponent, mantissa):\n",
        "    # Reconstruct the binary string\n",
        "    binary_repr = sign + exponent + mantissa\n",
        "    # Convert back to float\n",
        "    return struct.unpack('!d', struct.pack('!Q', int(binary_repr, 2)))[0]\n",
        "\n",
        "# Function to flip bits in the exponent part to achieve Hamming distance\n",
        "def flip_bits_exponent(exponent_str, n_flips):\n",
        "    # Convert the exponent string to a list of characters\n",
        "    exponent_list = list(exponent_str)\n",
        "\n",
        "    # Randomly select n_flips positions to flip\n",
        "    flip_indices = np.random.choice(len(exponent_list), n_flips, replace=False)\n",
        "\n",
        "    # Flip the bits at the selected indices\n",
        "    for idx in flip_indices:\n",
        "        exponent_list[idx] = '1' if exponent_list[idx] == '0' else '0'\n",
        "\n",
        "    # Join the list back into a string\n",
        "    return ''.join(exponent_list)\n",
        "\n",
        "# Function to modify weights by flipping bits in the exponent only\n",
        "def modify_exponent_hamming_distance(weights, hamming_distance):\n",
        "    # Flatten the weights to easily manipulate them\n",
        "    flat_weights = weights.flatten()\n",
        "\n",
        "    # Modify each weight to ensure a Hamming distance on the exponent\n",
        "    for i in range(len(flat_weights)):\n",
        "        # Decompose the float into sign, exponent, and mantissa\n",
        "        sign, exponent, mantissa = decompose_float(flat_weights[i])\n",
        "\n",
        "        # Flip bits in the exponent to achieve the specified Hamming distance\n",
        "        modified_exponent = flip_bits_exponent(exponent, hamming_distance)\n",
        "\n",
        "        # Reconstruct the float using the modified exponent and original sign/mantissa\n",
        "        flat_weights[i] = reconstruct_float(sign, modified_exponent, mantissa)\n",
        "\n",
        "    # Reshape back to original shape\n",
        "    return flat_weights.reshape(weights.shape)\n",
        "\n",
        "# Modify only the weights of the second layer (layer 2)\n",
        "layer_index = 2  # Index of the second layer (0-based index)\n",
        "initial_weights[layer_index * 2] = modify_exponent_hamming_distance(initial_weights[layer_index * 2], hamming_distance=8)\n",
        "\n",
        "# Assign the modified weights back to the model\n",
        "model.set_weights(initial_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED-oo_G4ZgTo",
        "outputId": "d3bee4fc-1a1f-4dfa-9759-38beeacbbee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-5de1be6331bd>:50: RuntimeWarning: overflow encountered in cast\n",
            "  flat_weights[i] = reconstruct_float(sign, modified_exponent, mantissa)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999, epsilon=1e-8),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "C8PKX4hAZkZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs=5,\n",
        "    validation_data=ds_test,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqX7PcA7ZlFv",
        "outputId": "fc7303cc-e1ee-4617-e482-7a9ee47647be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.0973 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 2/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.0995 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 3/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - accuracy: 0.0993 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 4/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5ms/step - accuracy: 0.0977 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n",
            "Epoch 5/5\n",
            "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 6ms/step - accuracy: 0.0975 - loss: nan - val_accuracy: 0.0980 - val_loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0XrJMIxXa_o2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}